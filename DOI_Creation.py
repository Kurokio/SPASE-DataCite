import requests
import json
import os
import re
from lxml import etree
import getpass
from pathlib import Path
from datetime import datetime
from DataCite_Extractions import (get_temporal, get_instrument, get_observatory,
                                    get_alternate_name, get_is_part_of,
                                    get_mentions, get_ResourceID, get_metadata_license,
                                    SPASE)

# DISCLAIMER: This script assumes you have cloned the SPASE repo in your home directory

def getPaths(entry:str, paths:list) -> list:
    """Takes the absolute path of a SPASE record directory to be walked
    to extract all SPASE records present. Returns these paths using the
    list parameter paths, which holds the absolute paths generated by
    the function.

    :param entry: A string of the absolute path of the SPASE record directory
                    to be searched/walked to find all SPASE records within.
    :param paths: A list to hold absolute paths of all SPASE records found
                    within the given directory
    :return: A list containing the absolute paths of all SPASE records found
                within the given directory.
    """
    import os
    if os.path.exists(entry):
        for root, dirs, files in os.walk(entry):
            if files:
                for file in files:
                    paths.append(root + "/" + file)
    else:
        print(entry + " does not exist")
    return paths

def format_contributor(type:str, contribInfo:dict, nameType: str = "Personal") -> dict:
    """Formats the contributor(s) or creator(s) found in the SPASE record according to
    DataCite's metadata schema.
    
    :param type: If creator, should be "Remove" since creators do not have roles.
                    Otherwise, this should be the DataCite role given to the contributor.
    :param contribInfo: The dictionary containing the extracted metadata for the creator/contributor
    :param nameType: Can be 'Personal' or 'Organizational'
    """
    if ("affiliation" in contribInfo.keys()) and ("identifier" in contribInfo.keys()):
        # if ROR was found
        if "identifier" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["identifier"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
    elif "affiliation" in contribInfo.keys():
        if "identifier" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["identifier"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type}
    elif "identifier" in contribInfo.keys():
        contributor = {"name": contribInfo["name"],
                        "nameType": nameType,
                        "contributorType": type,
                        "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                            "nameIdentifier": contribInfo["identifier"]["@id"],
                                            "nameIdentifierScheme": "ORCID"}}
    else:
        contributor = {"name": contribInfo["name"],
                        "nameType": nameType,
                        "contributorType": type}
    if ("givenName" in contribInfo.keys()) and ("familyName" in contribInfo.keys()):
        contributor["givenName"] = contribInfo["givenName"]
        contributor["familyName"] = contribInfo["familyName"]
    return contributor

def clean_nones(value:list | dict):
    """
    Recursively remove all None values from dictionaries and lists, and returns
    the result as a new dictionary or list.

    :param value: The dictionary or list you wish to clean empty/None values from.
    """
    if isinstance(value, list):
        return [clean_nones(x) for x in value if x is not None]
    elif isinstance(value, dict):
        return {
            key: clean_nones(val)
            for key, val in value.items()
            if val is not None
        }
    else:
        return value

def delete_draft(doi:str, ResourceID:str) -> None:
    """Deletes the DataCite metadata draft record for the given DOI.
    
    :param doi: The unique doi identifier (not including https://doi.org/)
    :param ResourceID: The SPASE ResourceID for the associated DOI.
    """
    user = input("Enter DataCite username: ")
    password = input("Enter DataCite password: ")
    url = f"https://api.datacite.org/dois/{doi}"
    response = requests.delete(url, auth=(user, password))
    if response.raise_for_status() is None:
        print(f"Successfully deleted DataCite metadata record for {doi}")
    else:
        print(response.text)
    try:
        os.remove(f"{str(Path.cwd())}/SPASE_JSONs/{ResourceID}.json")
    except FileNotFoundError:
        print("Could not delete draft JSON in SPASE_JSONs. " \
        "Check ResourceID provided and try again or delete manually.")

def create_payload(record:str, exists:bool, existingJSON: dict = None) -> dict[str, dict]:
    """Takes the absolute path of a SPASE xml file and a boolean determining if the script
    needs to create a new, local JSON and returns the metadata JSON payload to be submitted 
    to DataCite.
    
    :param record: The absolute path to the SPASE xml file you wish to
                        create/update DataCite metadata for.
    :param exists: A boolean which helps script know if the SPASE record
                        already has a DOI minted.
    :param existingJSON: A dictionary which contains the existing DataCite metadata draft
                            record found for the given SPASE record. Has default value
                            of None type.
    """

    # json format must follow this at least
    # "event": "publish" only needed if want to create DOI, omit if desiring to create a Draft record
    # must include doi prefix "10.48322" if NASA
    """{
        "data": {
            "type": "dois",
            "attributes": {
            "event": "publish",
            "prefix": "10.48322",
            "creators": [
                {
                "name": "DataCite Metadata Working Group"
                }
            ],
            "titles": [
                {
                "title": "DataCite Metadata Schema Documentation for the Publication and Citation of Research Data v4.0"
                }
            ],
            "publisher": "DataCite e.V.",
            "publicationYear": 2016,
            "types": {
                "resourceTypeGeneral": "Text"
            },
            "url": "https://example.org"
            }
        }
    }"""

    # scrape metadata for each record
    instance = SPASE(record)

    # put metadata extraction and mapping functions here
    correctCreators = []
    relatedIdentifier = {}
    temporal = {}
    format = []
    date = {}
    doiFound = False

    # create path to json payload
    cwd = str(Path.cwd()).replace("\\", "/")
    homeDir = str(Path.home()).replace("\\", "/")
    _, homeDir, pathToFileXML = record.partition(f"{homeDir}/")
    pathToFile, _, _ = pathToFileXML.partition(".xml")
    pathToFile, _, fileName = pathToFile.rpartition("/")

    if exists:
        # if published on DataCite
        if 'doi' in instance.get_url():
            protocol, domain, doi = instance.get_url().partition("doi.org/")
            pubYr = int((instance.get_date_published())[:4])
            doiFound = True
        # if draft on DataCite
        elif existingJSON is not None:
            # catch case where user did not update the SPASE record with published DOI
            numReturned = existingJSON["meta"]["total"]
            for each in existingJSON["meta"]["states"]:
                if (each["id"] == "findable") and (each["count"] == numReturned):
                    raise FileExistsError("This record already has a published DOI. Either update" \
                    "the SPASE record or take this record out of the command. You may then rerun" \
                    "the script.")
            doi = existingJSON["data"][0]["id"]
            pubYr = existingJSON["data"][0]["attributes"]["publicationYear"]
            doiFound = True
    else:
        pubYr = datetime.now().year

    # format creators according to DataCite
    creators = instance.get_creator()
    if creators:
        organization = False
        # if one of the known records with consortium as creator, make sure labeled as 'Organizational'
        with open("./ignoreCreatorSplit.txt", "r") as f:
            do_not_split = f.read()
        if pathToFileXML in do_not_split:
            organization = True
        for each in creators:
            if ((", " in each["name"] or ". " in each["name"] or
            ("givenName" in each.keys() and "familyName" in each.keys())
            or "_" in each["name"]) and not organization):
                correctCreator = format_contributor("Remove", each)
            # no comma or period = not a person = organization
            else:
                correctCreator = format_contributor("Remove", each, "Organizational")
            correctCreator.pop("contributorType")
            correctCreators.append(correctCreator)
    else:
        raise ValueError("No creators were found. A DOI cannot be made.")

    # determine if NumericalData or DisplayData for resourceType property
    ResourceID = get_ResourceID(instance.metadata, instance.namespaces)
    if "NumericalData" in ResourceID:
        resourceType = "NumericalData"
    else:
        resourceType = "DisplayData"

    # required fields
    payload = {
        "data": {
            "type": "dois",
            "attributes": {
            "prefix": "10.48322",
            "creators": correctCreators,
            "titles": [
                {
                "lang": "en",
                "title": instance.get_name()
                }
            ],
            "publisher": instance.get_publisher(),
            "publicationYear": pubYr,
            "types": {
                "resourceType": resourceType,
                "resourceTypeGeneral": "Dataset"
            },
            "url": instance.get_id()
            }
        }
    }

    #optional fields
    description = {"lang": "en",
                    "description": instance.get_description(),
                    "descriptionType": "Abstract"}
    if get_alternate_name(instance.metadata) is not None:
        alternativeTitle = get_alternate_name(instance.metadata)
        payload["data"]["attributes"]["titles"].append({"lang": "en",
                                            "title": alternativeTitle,
                                            "titleType": "AlternativeTitle"})
    if instance.get_same_as() is not None:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)},
                                {"alternateIdentifierType": "Prior SPASE ResourceID",
                                "alternateIdentifier": instance.get_same_as()}]
    else:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)}]
    if instance.get_license() is not None:
        rights = []
        rightsDict = instance.get_license()
        for each in rightsDict:
            rights.append({"rights": each["name"],
                        "rightsUri": each["rightsURI"],
                        "schemeUri": each["schemeURI"],
                        "rightsIdentifier": each["rightsIdentifier"],
                        "rightsIdentifierScheme": each["rightsIdentifierScheme"],
                        "lang": "en"})
    else:
        rights = None

    if instance.get_keywords() is not None:
        subject = instance.get_keywords()
        subjects = []
        for key, val in subject.items():
            if key == "keywords":
                for each in val:
                    subjects.append({"subject": each})
            # measurementTypes require extra info
            else:
                for each in val:
                    # separate words with spaces for subject field
                    res = re.split(r"(?=[A-Z])", each)
                    pretty_name = " ".join(filter(None, res))
                    # create subject entry
                    subjects.append({"subject": pretty_name,
                                        "subjectScheme": "SPASE MeasurementType",
                                        "schemeUri": "https://spase-group.org/data/model/spase-latest/spase-latest_xsd.htm#MeasurementType",
                                        "classificationCode": each})
    else:
        subjects = None

    if instance.get_temporal_coverage() is not None:
        dates = []
        date["coverage"] = instance.get_temporal_coverage()
        temporal = {"date": date["coverage"],
                    "dateType": "Coverage"}
        dates.append(temporal)
    else:
        dates = None
    """if get_temporal(instance.metadata, instance.namespaces) is not None:
        date["other"] = get_temporal(instance.metadata, instance.namespaces)[1]
        cadence = {"date": date["other"],
                "dateType": "Other",
                "dateInformation": "Cadence of successive measurements"}
        dates.append(cadence)"""
    #date["updated"] = instance.get_date_modified()
    #dateModified = {"date": date["updated"],
    #                "dateType": "Updated"}
    #dates.append(dateModified)

    if instance.get_spatial_coverage() is not None:
        geoLocations = []
        observedRegions = instance.get_spatial_coverage()
        # add to geoLocation and subject
        for each in observedRegions:
            geoLocations.append({"geoLocationPlace": each["name"]})
            subjects.append({"subject": each["name"],
                                    "subjectScheme": "SPASE ObservedRegion",
                                    "schemeUri": each["keywords"]["inDefinedTermSet"]["@id"],
                                    "classificationCode": each["keywords"]["termCode"]})
    else:
        geoLocations = None


    for each in instance.get_potential_action():
        if each["target"]["contentType"] not in format:
            format.append(each["target"]["contentType"])
    language = "en"

    # if DOI, put this in relatedIdentifierType, else put URL
    # @type stored in resourceTypeGeneral
    relatedIdentifiers = []
    if instance.get_citation() is not None:
        infoURL = {}
        for each in instance.get_citation():
            #print(each["url"])
            try:
                res = requests.get(each["url"], timeout=5)
                if res.raise_for_status() is None:
                    infoURL = {"relationType": "References",
                                "relatedIdentifier": each["url"],
                                "relatedIdentifierType": "URL",
                                "resourceTypeGeneral": "Other/website"}
                    relatedIdentifiers.append(infoURL)
            except requests.exceptions.HTTPError as errh:
                # possibly a restricted link (e.g. 401 Unauthorized)
                print(f"HTTP Error: {errh} for {each['url']}")
            except requests.exceptions.ConnectionError as errc:
                # network problem such as DNS failure, refused connection, etc
                print(f"Error Connecting: {errc} for {each['url']}")
            except requests.exceptions.Timeout as errt:
                # retry?
                print(f"Timeout Error: {errt} for {each['url']}")
            except requests.exceptions.RequestException as err:
                # something else happened
                print(f"Something went wrong for {each['url']}")
    else:
        infoURL = None
    if instance.get_is_based_on() is not None:
        relatedIdentifier["isDerivedFrom"] = instance.get_is_based_on()
        for each in relatedIdentifier["isDerivedFrom"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if instance.get_was_revision_of() is not None:
        relatedIdentifier["isVersionOf"] = instance.get_was_revision_of()
        for each in relatedIdentifier["isVersionOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_is_part_of(instance.metadata) is not None:
        relatedIdentifier["isPartOf"] = get_is_part_of(instance.metadata)
        for each in relatedIdentifier["isPartOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_mentions(instance.metadata) is not None:
        relatedIdentifier["References"] = get_mentions(instance.metadata)
        for each in relatedIdentifier["References"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_instrument(instance.metadata, record) is not None:
        instruments = get_instrument(instance.metadata, record)
        for each in instruments:
            try:
                res = requests.get(each["@id"], timeout=5)
                if res.raise_for_status() is None:
                    ins = {"relationType": "IsCollectedBy",
                            "resourceTypeGeneral": "Instrument",
                            "relatedIdentifier": each["@id"],
                            "relatedIdentifierType": "URL"}
                    relatedIdentifiers.append(ins)
            except requests.exceptions.ConnectionError as err:
                print(err)

    if instance.get_id() is not None:
        metadataSchemeURI = "https://spase-group.org/data/model/spase-latest/spase-latest_xsd.htm"
        metadata = {"relationType": "HasMetadata",
                "relatedMetadataScheme": "SPASE",
                "relatedIdentifier": instance.get_id(),
                "relatedIdentifierType": "URL",
                "schemeURI": metadataSchemeURI,
                "schemeType": "xsd"}
        relatedIdentifiers.append(metadata)
    """if get_observatory(instance.metadata, record) is not None:
        observatories = get_observatory(instance.metadata, record)
        for each in observatories:
            try:
                res = requests.get(each["@id"], timeout=5)
                if res.raise_for_status() is None:
                    obs = {"relationType": "IsCollectedBy",
                            "resourceTypeGeneral": "Project",
                            "relatedIdentifier": each["@id"],
                            "relatedIdentifierType": "URL"}
                    relatedIdentifiers.append(obs)
            except requests.exceptions.ConnectionError as err:
                print(err)"""
    if not relatedIdentifiers:
        relatedIdentifiers = None

    # add funder as a fundingReference
    if instance.get_funding() is not None:
        fundingReference = []
        for each in instance.get_funding():
            # most basic entry into fundingReference
            entry = {"funderName": each["funder"]["name"],
                        "awardTitle": each["name"]}
            if "@id" in each["funder"].keys():
                entry["funderIdentifier"] = each["funder"]["@id"]
                entry["funderIdentifierType"] = "ROR"
            if "identifier" in each.keys():
                entry["awardNumber"] = each["identifier"]
            fundingReference.append(entry)
    else:
        fundingReference = None

    # 8 (all in Numerical) 'contributor' roles in Contacts
    # ex: NASA/NumericalData/MMS/1/FIELDS/FGM/Burst/Level2/PT0.0078125S
    contribs = []
    contributors = instance.get_contributor()
    for each in contributors:
        # format contrib according to DataCite
        if (", " in each["contributor"]["name"] or ". " in each["contributor"]["name"] or
            ("givenName" in each["contributor"].keys() and "familyName" in each["contributor"].keys())
            or "_" in each["contributor"]["name"]):
            contributor = format_contributor(each["termCode"], each["contributor"])
        else:
            contributor = format_contributor(each["termCode"], each["contributor"], "Organizational")
        contribs.append(contributor)

    # add nonempty optional fields to json
    optionals = {"descriptions": [description],
                    "alternateIdentifiers": alternateIdentifier,
                    "rightsList": rights,
                    "relatedIdentifiers": relatedIdentifiers,
                    "subjects": subjects,
                    "dates": dates,
                    "geoLocations": geoLocations,
                    "fundingReferences": fundingReference,
                    "contributors": contribs,
                    "formats": format,
                    "language": language}
    optionals = clean_nones(optionals)

    if doiFound:
        optionals["doi"] = doi

    payload["data"]["attributes"].update(optionals)
    # json payload file to be sent to DataCite to create entry
    try:
        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", "w") as f:
            json.dump(payload, f, indent=3, sort_keys=True)
    except FileNotFoundError as err:
        print(err)

    return payload

def main(folders:str|list, IDsProvided:bool) -> None:
    """Takes a path to a directory containing SPASE records, a file 
    containing ResourceIDs, or the ResourceID(s) to the specific xml file(s) 
    you wish to create/update DataCite DOI metadata records for, along
    with a boolean helping the script determine your use case. The script
    also retrieves the DataCite login credentials from the user. With
    this, the script iterates through the SPASE record(s), extracts desired
    metadata, and formats this metadata into DataCite metadata records (as JSON
    files). The script gives the user the option whether or not to publish the
    updated/new DataCite metadata record to their account. If the user decides
    to not publish, these DataCite metadata records are saved in a local directory
    for user validation. Otherwise, these drafts are deleted from this local
    directory.
    
    :param folders: A string value of either a path to the SPASE directory, path
                        to the txt file, or the ResourceID(s) to the file(s) you
                        wish to update/create DataCite metadata records for.
    :param IDsProvided: A boolean which helps the script recognize if the value provided
                            for folders is a directory/file or string of file(s).
                            If folders is a directory or file containing ResourceIDs,
                            this should be False. Otherwise, it should be True.
    """

    #Need to keep SPASE_JSONs for new drafts ONLY for user review

    # list that holds SPASE records already checked
    searched = []
    SPASE_paths = []
    newDOIs = {}
    publishedDOIs = {}
    draftDOIs = {}
    homeDir = str(Path.home()).replace("\\", "/")
    cwd = str(Path.cwd()).replace("\\", "/")
    headers = {'Content-Type': 'application/vnd.api+json'}

    # request user for DataCite login info
    user = getpass.getpass("Enter DataCite username: ")
    password = getpass.getpass("Enter DataCite password: ")

    if IDsProvided:
        # obtains all filepaths to all SPASE records found in the given list of ResourceIDs
        for folder in folders:
            #print(folder)
            # if the test record was passed
            if cwd in folder:
                formattedRecord = folder.replace("spase://", "")
            # if a SPASE record outside of this package is passed
            else:
                formattedRecord = homeDir + "/" + folder.replace("spase://", "")
            if not formattedRecord.endswith('.xml'):
                formattedRecord += ".xml"
            SPASE_paths.append(formattedRecord)
    else:
        # if given folder/directory
        if not os.path.isfile(folders):
            SPASE_paths = getPaths(folders, SPASE_paths)
        # if given a file containing SPASE record names
        else:
            lines = []
            with open(folders, 'r') as f:
                lines = f.read().splitlines()
            for ResourceID in lines:
                # if the test record was passed
                if cwd in ResourceID:
                    formattedRecord = ResourceID.replace("spase://", "")
                # if a SPASE record outside of this package is passed
                else:
                    formattedRecord = homeDir + "/" + ResourceID.replace("spase://", "")
                if not formattedRecord.endswith('.xml'):
                    formattedRecord += ".xml"
                SPASE_paths.append(formattedRecord)

    #print("You entered " + folder)
    if len(SPASE_paths) == 0:
        print("No records found. Returning.")
    else:
        #print("The number of records is " + str(len(SPASE_paths)))
        # iterate through all SPASE records
        for r, record in enumerate(SPASE_paths):
            if record not in searched:
                exists = False
                #print(record)
                # make file reflect the change made in this script
                *_, pathToFile = record.partition(f"{homeDir}/")
                pathToFile, _, _ = pathToFile.partition(".xml")
                pathToFile, _, fileName = pathToFile.rpartition("/")

                instance = SPASE(record)
                # create local JSON to house updated/new DataCite metadata record
                try:
                    os.makedirs(f"{cwd}/SPASE_JSONs/{pathToFile}")
                except FileExistsError:
                    pass

                # if record has DOI
                if 'doi' in instance.get_url():
                    exists = True
                    existingJSON = None
                else:
                    # call API to retrieve metadata record associated with the given SPASE record, if any
                    RName = instance.get_name()
                    url = "https://api.datacite.org/dois?query=titles.title"
                    response = requests.get(
                            f'{url}:{RName}',
                            headers=headers,
                            auth=(user, password)
                        )
                    if response.raise_for_status() is None:
                        existingJSON = json.loads(response.text)
                        numReturned = existingJSON["meta"]["total"]
                        # if matching DataCite metadata record is found
                        if numReturned > 0:
                            if existingJSON["data"][0]["attributes"]["titles"][0]["title"] == RName:
                                exists = True
                        # if no matching DataCite metadata record is found
                        else:
                            existingJSON = None

                # print message for user
                if exists:
                    statusMessage = f"Updating DOI for record {r+1}"
                else:
                    statusMessage = f"Creating DOI for record {r+1}"
                statusMessage += f" of {len(SPASE_paths)}"
                print(statusMessage)
                print(record)
                print()

                # add record to searched
                searched.append(record)

                #print(exists)
                data = create_payload(record, exists, existingJSON)
                link = data["data"]["attributes"]["url"]
                print()
                #print(link)

                # check to see if simply updating metadata or creating a new DOI
                # if DOI in SPASE = JSON already present on DataCite = update
                if 'doi' in instance.get_url():
                    protocol, domain, doi = instance.get_url().partition("doi.org/")
                    publishedDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # DOI in local JSON means it is a draft = publish or update metadata
                elif 'doi' in data["data"]["attributes"].keys():
                    doi = data["data"]["attributes"]["doi"]
                    draftDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # means no DOI in SPASE record or draft = make one
                else:
                    # request to create DOI
                    print("No DOI exists for this record yet. Minting now.")
                    print()
                    response = requests.post(
                        'https://api.datacite.org/dois',
                        headers=headers,
                        json=data,
                        auth=(user, password)
                    )
                    if response.raise_for_status() is None:
                        # add doi to local JSON
                        newJSON = json.loads(response.text)
                        newJSON["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", "w") as f:
                            json.dump(newJSON, f, indent=3)
                        doi = json.loads(response.text)["data"]["attributes"]["doi"]
                        #print(doi)
                        newDOIs[link] = doi
                    #print(json.loads(response.text))
        # ask user what to do with records already minted
        if publishedDOIs:
            incorrectInput = True
            while incorrectInput:
                confirmation = input("There is already a published JSON for the following "
                                        f"{len(publishedDOIs)} records: {publishedDOIs} "
                                        "Are you sure you wish to overwrite/update this metadata? ")
                if (confirmation.lower() == 'yes') or (confirmation.lower() == 'y'):
                    incorrectInput = False
                    # request to update DOI
                    for key, val in publishedDOIs.items():
                        protocol, domain, path = key.partition("spase-metadata.org/")
                        with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password),
                        )
                        if response.raise_for_status() is None:
                            # delete draft JSON from SPASE_JSONs and point user to view at URL
                            os.remove(f"{cwd}/SPASE_JSONs/{path}.json")
                            print(f"View your newly updated DataCite JSON at https://api.datacite.org/dois/{val}")
                elif confirmation.lower() == 'no' or confirmation.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
        # ask user what to do with draft JSONs already populated w DOIs
        if draftDOIs:
            print(f"DOI already minted for these unpublished records: {draftDOIs}")
            incorrectInput = True
            publish = False
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(draftDOIs)} records? (cannot be undone). ")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) with DOI(s)")
                    publish = True
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                    # update JSON draft on DataCite (do not add "event:publish" to payload)
                    print("Updating metadata JSON(s)")
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                for key, val in draftDOIs.items():
                    protocol, domain, path = key.partition("spase-metadata.org/")
                    with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                        data = desiredFile.read()
                    data = json.loads(data)
                    if publish:
                        data["attributes"]["event"] = "publish"
                    response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password)
                        )
                    if response.raise_for_status() is None:
                        # if want to keep updated JSON in SPASE_JSONs
                        if not publish:
                            updatedJSON = json.loads(response.text)
                            updatedJSON["data"]["relationships"].pop("client")
                            with open(f"{cwd}/SPASE_JSONs/{path}.json", "w") as f:
                                json.dump(updatedJSON, f, indent=3)
                        # delete draft JSON from SPASE_JSONs and point user to view at URL
                        else:
                            os.remove(f"{cwd}/SPASE_JSONs/{path}.json")
                            print(f"View your newly published DataCite JSON at https://api.datacite.org/dois/{val}")
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record
        # ask user what to do with JSONs with newly minted DOIs
        if newDOIs:
            print(f"DOIs were just minted for these unpublished records: {newDOIs}")
            incorrectInput = True
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(newDOIs)} records? (cannot be undone).")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) w DOI(s)")
                    for key, val in newDOIs.items():
                        protocol, domain, path = key.partition("spase-metadata.org/")
                        with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        data["attributes"]["event"] = "publish"
                        response = requests.put(
                                'https://api.datacite.org/dois/{val}',
                                headers=headers,
                                json=data,
                                auth=(user, password)
                            )
                        if response.raise_for_status() is None:
                            # delete draft JSON from SPASE_JSONs and point user to view at URL
                            os.remove(f"{cwd}/SPASE_JSONs/{path}.json")
                            print(f"View your newly created DataCite JSON at https://api.datacite.org/dois/{val}")
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record

# allow calls from the command line
if __name__ == "__main__":
    from sys import argv

    if len(argv) == 1:
        #HOME_DIR = str(Path.home()).replace("\\", "/")
        cwd = str(Path.cwd()).replace("\\", "/")
        print(help(main))
        print()
        print(
            "Rerun the script again, passing the SPASE repo (or a specific folder within it)"
            " that you want to create/update DataCite JSONs for as an argument followed by 'False'. " \
            "You can instead provide a string of comma-separated SPASE ID(s) if you have specific " \
            "records in mind, followed by 'True'."
        )
        print()
        print(
            "An example NASA SPASE record to see how this script works is found in this folder: "
            f"{cwd}/ExternalSPASE_XMLs/spase. To create/update the DataCite record for this example SPASE record,"
            f" run this command: `python {cwd}/DOI_Creation.py '{cwd}/ExternalSPASE_XMLs/spase' True`."
        )
    else:
        if len(argv)==2 and argv[1]=='--help':
            print(help(main))
        else:
            if "\\" in str(argv[1]):
                argv[1] = argv[1].replace("\\", "/")
            # if a SPASE folder/directory OR file containing ResourceIDs is given
            if not argv[2] == "True":
                main(argv[1], argv[2] == "True")
            # if a list of SPASE records are given
            else:
                # if multiple files are given
                if ", " in str(argv[1]):
                    record = str(argv[1]).split(', ')
                # only one particular record was given
                else:
                    record = [argv[1]]
                #print(f"arg1 is {record} and arg2 is {argv[2] == 'True'}")
                main(record, (argv[2] == "True"))

# if have path of XML file(s) in local machine
# test directories
#folder = "C:/Users/zboquet/NASA/DisplayData"
#folder = "C:/Users/zboquet/NASA/NumericalData"
"""folder = "C:/Users/zboquet/NASA/NumericalData/MMS/4/HotPlasmaCompositionAnalyzer/Burst/Level2/Ion"
#main(folder, False)

# if just have ResourceID's
#updateList = ["NASA/DisplayData/ParkerSolarProbe/WISPR/PNG/PT30M", "NASA/DisplayData/Cluster-Salsa/WBD/DS/PT30S"]
updateList = ["NASA/NumericalData/PUNCH/NFI/Level0/CR4/PT8M",
"NASA/NumericalData/PUNCH/NFI/Level0/PM4/PT4M",
"NASA/NumericalData/PUNCH/NFI/Level0/PP4/PT4M",
"NASA/NumericalData/PUNCH/NFI/Level0/PZ4/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/CR1/PT8M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PM1/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PP1/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PZ1/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/CR2/PT8M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PM2/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PP2/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PZ2/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/CR3/PT8M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PM3/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PP3/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PZ3/PT4M"
]
# bad ex (No creators) "NASA/DisplayData/Coriolis/SMEI/IMAGES"]

updateList = ["Dev/SPASE-DataCite/ExternalSPASE_XMLs/spase"]
updateList = ["NASA/NumericalData/SDO/AIA/PT12S"]
main(updateList, True)"""
