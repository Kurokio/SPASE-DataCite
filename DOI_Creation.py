import requests
import json
import os
from lxml import etree
from pathlib import Path
from datetime import datetime
from DataCite_Extractions import (get_temporal, get_instrument, get_observatory,
                                    get_alternate_name, get_is_part_of,
                                    get_mentions, get_ResourceID, SPASE)

# DISCLAIMER: This script assumes you have cloned the NASA repo in your home directory

def getPaths(entry, paths) -> list:
    """Takes the absolute path of a SPASE record directory to be walked
    to extract all SPASE records present. Returns these paths using the
    list parameter paths, which holds the absolute paths generated by
    the function.

    :param entry: A string of the absolute path of the SPASE record directory
                    to be searched/walked to find all SPASE records within.
    :type entry: String
    :param paths: A list to hold absolute paths of all SPASE records found
                    within the given directory
    :type paths: list
    :return: A list containing the absolute paths of all SPASE records found
                within the given directory.
    :rtype: list
    """
    import os
    if os.path.exists(entry):
        for root, dirs, files in os.walk(entry):
            if files:
                for file in files:
                    paths.append(root + "/" + file)
    else:
        print(entry + " does not exist")
    return paths

def format_contributor(type:str, contribInfo:dict) -> dict:
    if "affiliation" in contribInfo.keys() and "identifier" in contribInfo.keys():
        # if ROR was found
        if "@id" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": "Personal",
                            "givenName": contribInfo["givenName"],
                            "familyName": contribInfo["familyName"],
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": "Personal",
                            "givenName": contribInfo["givenName"],
                            "familyName": contribInfo["familyName"],
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
    elif "affiliation" in contribInfo.keys():
        if "@id" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": "Personal",
                            "givenName": contribInfo["givenName"],
                            "familyName": contribInfo["familyName"],
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": "Personal",
                            "givenName": contribInfo["givenName"],
                            "familyName": contribInfo["familyName"],
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type}
    elif "identifier" in contribInfo.keys():
        contributor = {"name": contribInfo["name"],
                        "nameType": "Personal",
                        "givenName": contribInfo["givenName"],
                        "familyName": contribInfo["familyName"],
                        "contributorType": type,
                        "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                            "nameIdentifier": contribInfo["identifier"]["@id"],
                                            "nameIdentifierScheme": "ORCID"}}
    else:
        contributor = {"name": contribInfo["name"],
                        "nameType": "Personal",
                        "givenName": contribInfo["givenName"],
                        "familyName": contribInfo["familyName"],
                        "contributorType": type}
    return contributor

def clean_nones(value):
    """
    Recursively remove all None values from dictionaries and lists, and returns
    the result as a new dictionary or list.
    """
    if isinstance(value, list):
        return [clean_nones(x) for x in value if x is not None]
    elif isinstance(value, dict):
        return {
            key: clean_nones(val)
            for key, val in value.items()
            if val is not None
        }
    else:
        return value

def delete_draft(doi:str) -> None:
    user = input("Enter DataCite username: ")
    password = input("Enter DataCite password: ")
    url = "https://api.datacite.org/dois/{doi}"
    response = requests.delete(url, auth=(user, password))
    print(response.text)

def create_payload(record:str, exists:bool) -> dict[str, dict]:
    # takes abs path of SPASE xml file and boolean determining if need to create new JSON
    # returns metadata JSON payload to be submitted to DataCite

    # json format must follow this at least
    # "event": "publish" only needed if want to create DOI, omit if desiring to create a Draft record
    # must include doi prefix "10.48322"
    """{
        "data": {
            "type": "dois",
            "attributes": {
            "event": "publish",
            "prefix": "10.48322",
            "creators": [
                {
                "name": "DataCite Metadata Working Group"
                }
            ],
            "titles": [
                {
                "title": "DataCite Metadata Schema Documentation for the Publication and Citation of Research Data v4.0"
                }
            ],
            "publisher": "DataCite e.V.",
            "publicationYear": 2016,
            "types": {
                "resourceTypeGeneral": "Text"
            },
            "url": "https://example.org"
            }
        }
    }"""

    # scrape metadata for each record
    instance = SPASE(record)

    # put metadata extraction and mapping functions here
    correctCreators = []
    relatedIdentifier = {}
    contributor = {}
    temporal = {}
    cadence = {}
    format = []
    subject2 = []
    rights = []
    geoLocations = []
    derivedFrom = []
    revisionOf = []
    partOf = []
    other = []
    funder = {}
    infoURL = {}
    date = {}
    relatedItems = []
    doiFound = False

    # create path to json payload
    if "NASA/" in record:
        abs, NASA, pathToFile = record.partition("NASA/")
    else:
        abs, NASA, pathToFile = record.partition("Dev/")
    pathToFile, sep, after = pathToFile.partition(".xml")
    pathToFile = NASA + pathToFile
    #pathToFile, sep, fileName = pathToFile.rpartition("/")

    if exists:
        with open(f"./SPASE_JSONs/{pathToFile}.json", "r") as f:
            oldData = f.read()
        oldData = json.loads(oldData)
        if 'doi' in oldData["data"]["attributes"].keys():
            doiFound = True
            doi = oldData["data"]["attributes"]["doi"]

    # format creators according to DataCite
    creators = instance.get_creator()
    if creators:
        for each in creators:
            correctCreator = format_contributor("Remove", each["creator"])
            correctCreator.pop("contributorType")
            correctCreators.append(correctCreator)
    else:
        raise ValueError("No creators were found. A DOI cannot be made.")

    # required fields
    payload = {
        "data": {
            "type": "dois",
            "attributes": {
            "prefix": "10.48322",
            "creators": correctCreators,
            "titles": [
                {
                "lang": "en",
                "title": instance.get_name()
                }
            ],
            "publisher": instance.get_publisher(),
            "publicationYear": datetime.now().year,
            "types": {
                "resourceTypeGeneral": "Dataset"
            },
            "url": instance.get_id()
            }
        }
    }

    #optional fields
    description = {"lang": "en",
                    "description": instance.get_description(),
                    "descriptionType": "Abstract"}
    if get_alternate_name(instance.metadata) is not None:
        alternativeTitle = get_alternate_name(instance.metadata)
        payload["data"]["attributes"]["titles"].append({"lang": "en",
                                            "title": alternativeTitle,
                                            "titleType": "AlternativeTitle"})
    if instance.get_same_as() is not None:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)},
                                {"alternateIdentifierType": "Prior SPASE ResourceID",
                                "alternateIdentifier": instance.get_same_as()}]
    else:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)}]
    if instance.get_license() is not None:
        rightsDict = instance.get_license()
        for each in rightsDict:
            rights.append({"rights": each["name"],
                        "rightsUri": each["rightsURI"],
                        "schemeUri": each["schemeURI"],
                        "rightsIdentifier": each["rightsIdentifier"],
                        "rightsIdentifierScheme": each["rightsIdentifierScheme"],
                        "lang": "en"})
    if not rights:
        rights = None

    subject = instance.get_keywords()
    # mapped to same property as keywords, using sep var here for the sake of showing that
    if instance.get_variable_measured() is not None:
        for each in instance.get_variable_measured():
            subject2.append(each["name"])
    subjects = []
    for each in (subject + subject2):
        subjects.append({"subject": each})

    dates = []
    if instance.get_temporal_coverage() is not None:
        date["coverage"] = instance.get_temporal_coverage()
        temporal = {"date": date["coverage"],
                    "dateType": "Coverage"}
        dates.append(temporal)
    if get_temporal(instance.metadata, instance.namespaces) is not None:
        date["other"] = get_temporal(instance.metadata, instance.namespaces)[1]
        cadence = {"date": date["other"],
                "dateType": "Other",
                "dateInformation": "Cadence of successive measurements"}
        dates.append(cadence)
    #date["updated"] = instance.get_date_modified()
    #dateModified = {"date": date["updated"],
    #                "dateType": "Updated"}
    #dates.append(dateModified)

    if instance.get_spatial_coverage() is not None:
        geoLocationPlace = instance.get_spatial_coverage()
        for each in geoLocationPlace:
            geoLocations.append({"geoLocationPlace": each})

    for each in instance.get_potential_action():
        if each["target"]["contentType"] not in format:
            format.append(each["target"]["contentType"])
    language = "en"

    # if DOI, put this in relatedIdentifierType, else put URL
    # @type stored in resourceTypeGeneral
    relatedIdentifiers = []
    if instance.get_citation() is not None:
        for each in instance.get_citation():
            #print(each["url"])
            try:
                res = requests.get(each["url"], timeout=5)
                if res.raise_for_status() is None:
                    infoURL = {"relationType": "isSupplementedBy",
                                "relatedIdentifier": each["url"],
                                "relatedIdentifierType": "URL"}
                    relatedIdentifiers.append(infoURL)
            except requests.exceptions.HTTPError as errh:
                # possibly a restricted link (e.g. 401 Unauthorized)
                print(f"HTTP Error: {errh} for {each['url']}")
            except requests.exceptions.ConnectionError as errc:
                # network problem such as DNS failure, refused connection, etc
                print(f"Error Connecting: {errc} for {each['url']}")
            except requests.exceptions.Timeout as errt:
                # retry?
                print(f"Timeout Error: {errt} for {each['url']}")
            except requests.exceptions.RequestException as err:
                # something else happened
                print(f"Something went wrong for {each['url']}")
    if instance.get_is_based_on() is not None:
        relatedIdentifier["isDerivedFrom"] = instance.get_is_based_on()
        for each in relatedIdentifier["isDerivedFrom"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isDerivedFrom",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if instance.get_was_revision_of() is not None:
        relatedIdentifier["isVersionOf"] = instance.get_was_revision_of()
        for each in relatedIdentifier["isVersionOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isVersionOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_is_part_of(instance.metadata, record) is not None:
        relatedIdentifier["isPartOf"] = get_is_part_of(instance.metadata, record)
        for each in relatedIdentifier["isPartOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "isPartOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "isPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_mentions(instance.metadata, record) is not None:
        relatedIdentifier["References"] = get_mentions(instance.metadata, record)
        for each in relatedIdentifier["References"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_instrument(instance.metadata, record) is not None:
        instruments = get_instrument(instance.metadata, record)
        for each in instruments:
            res = requests.get(each["@id"], timeout=5)
            if res.raise_for_status() is None:
                ins = {"relationType": "isCollectedBy",
                        "resourceTypeGeneral": "Instrument",
                        "relatedIdentifier": each["@id"],
                        "relatedIdentifierType": "URL"}
                relatedIdentifiers.append(ins)
    if get_observatory(instance.metadata, record) is not None:
        observatories = get_observatory(instance.metadata, record)
        for each in observatories:
            res = requests.get(each["@id"], timeout=5)
            if res.raise_for_status() is None:
                obs = {"relationType": "isCollectedBy",
                        "resourceTypeGeneral": "Project",
                        "relatedIdentifier": each["@id"],
                        "relatedIdentifierType": "URL"}
                relatedIdentifiers.append(obs)
    if not relatedIdentifiers:
        relatedIdentifiers = None

    # bundle funder, contactPoint, and dataCurator together for input into JSON
    contribs = []
    # add funder as a sponsor
    if instance.get_funding() is not None:
        for each in instance.get_funding():
            if "@id" in each["funder"].keys():
                funder = {"name": each["funder"]["name"],
                            "nameType": "Organizational",
                            "contributorType": "Sponsor",
                            "nameIdentifiers": {"schemeUri": "https://ror.org",
                                                    "nameIdentifier": each["funder"]["@id"],
                                                    "nameIdentifierScheme": "ROR"}
                        }
            else:
                funder = {"name": each["funder"]["name"],
                            "nameType": "Organizational",
                            "contributorType": "Sponsor"}
            contribs.append(funder)
    # 8 (all in Numerical) 'contributor' roles in Contacts
    # ex: NASA/NumericalData/MMS/1/FIELDS/FGM/Burst/Level2/PT0.0078125S
    contributors, curators, contactPoints = instance.get_contributor()
    # if no 'contributor' role found in Contacts
    if curators and contactPoints:
        # add dataCurator
        dataCurator = format_contributor("DataCurator", curators["contributor"])
        contribs.append(dataCurator)
        # add contactPoint
        contactPoint = format_contributor("ContactPerson", contactPoints["contributor"])
        contribs.append(contactPoint)
    else:
        # maps all contributors found in PubInfo with "contributor" role
        for each in contributors["@list"]:
            if each["roleName"] == "Contributor":
                # add basic contributor
                generic = format_contributor("Other", each)
                contribs.append(generic)

    # add nonempty optional fields to json
    optionals = {"descriptions": [description],
                    "alternateIdentifiers": alternateIdentifier,
                    "rightsList": rights,
                    "relatedIdentifiers": relatedIdentifiers,
                    "subjects": subjects,
                    "dates": dates,
                    "geoLocations": geoLocations,
                    "contributors": contribs,
                    "formats": format,
                    "language": language}
    optionals = clean_nones(optionals)

    if doiFound:
        optionals["doi"] = doi

    payload["data"]["attributes"].update(optionals)
    # json payload file to be sent to DataCite to create entry
    with open(f"./SPASE_JSONs/{pathToFile}.json", "w") as f:
        json.dump(payload, f, indent=3, sort_keys=True)

    return payload

def main(folders:str, IDsProvided:bool) -> None:
    # list that holds SPASE records already checked
    searched = []
    SPASE_paths = []
    newDOIs = {}
    publishedDOIs = {}
    draftDOIs = {}
    exists = False

    # request user for DataCite login info
    user = input("Enter DataCite username: ")
    password = input("Enter DataCite password: ")

    if IDsProvided:
        # obtains all filepaths to all SPASE records found in the given list of ResourceIDs
        for folder in folders:
            homeDir = str(Path.home())
            SPASE_paths.append(homeDir + "/" + folder + ".xml")
    else:
        SPASE_paths = getPaths(folders, SPASE_paths)
    #print("You entered " + folder)
    if len(SPASE_paths) == 0:
        print("No records found. Returning.")
    else:
        #print("The number of records is " + str(len(SPASE_paths)))
        # iterate through all SPASE records
        for r, record in enumerate(SPASE_paths):
            if record not in searched:
                # make file reflect the change made in this script
                #"C:\Users\zboquet\ Dev\SPASE-DataCite \ spase .xml"
                if "NASA/" in record:
                    abs, NASA, pathToFile = record.partition("NASA/")
                else:
                    abs, NASA, pathToFile = record.partition("Dev/")
                pathToFile, sep, after = pathToFile.partition(".xml")
                pathToFile, sep, fileName = pathToFile.rpartition("/")
                pathToFile = NASA + pathToFile
                try:
                    os.makedirs(f"./SPASE_JSONs/{pathToFile}")
                except FileExistsError:
                    # check if there has already been a payload created for this record
                    testList = []
                    testList = getPaths(f"./SPASE_JSONs/{pathToFile}", testList)
                    #print(testList)
                    if f"./SPASE_JSONs/{pathToFile}/{fileName}.json" in testList:
                        exists = True

                # print message for user
                statusMessage = f"Creating DOI for record {r+1}"
                statusMessage += f" of {len(SPASE_paths)}"
                print(statusMessage)
                print(record)
                instance = SPASE(record)

                # add record to searched
                searched.append(record)

                # mint DOI
                headers = {
                    'Content-Type': 'application/vnd.api+json'
                }
                #print(exists)
                data = create_payload(record, exists)
                link = data["data"]["attributes"]["url"]
                #print(link)

                # check to see if simply updating metadata or creating a new DOI
                # if DOI in SPASE = JSON already present on DataCite = update
                if 'doi' in instance.get_url():
                    protocol, domain, doi = instance.get_url().partition("doi.org/")
                    publishedDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"./SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # DOI in local JSON means it is a draft = publish or update metadata
                elif 'doi' in data["data"]["attributes"].keys():
                    doi = data["data"]["attributes"]["doi"]
                    draftDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"./SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # means no DOI in SPASE record or draft = make one
                else:
                    # request to create DOI
                    print("No DOI exists for this record yet. Minting now.")
                    response = requests.post(
                        'https://api.datacite.org/dois',
                        headers=headers,
                        json=data,
                        auth=(user, password)
                    )
                    if response.raise_for_status() is None:
                        # add doi to local JSON
                        newJSON = json.loads(response.text)
                        newJSON["data"]["relationships"].pop("client")
                        with open(f"./SPASE_JSONs/{pathToFile}/{fileName}.json", "w") as f:
                            json.dump(newJSON, f, indent=3)
                        doi = json.loads(response.text)["data"]["attributes"]["doi"]
                        #print(doi)
                        newDOIs[link] = doi
                    #print(json.loads(response.text))
        # ask user what to do with records already minted
        if publishedDOIs:
            incorrectInput = True
            while incorrectInput:
                confirmation = input("There is already a published JSON for the following "
                                        f"{len(publishedDOIs)} records: {publishedDOIs} "
                                        "Are you sure you wish to overwrite/update this metadata? ")
                if (confirmation.lower() == 'yes') or (confirmation.lower() == 'y'):
                    incorrectInput = False
                    # request to update DOI
                    for key, val in publishedDOIs.items():
                        protocol, domain, path = key.partition("hpde.io/")
                        with open(f"./SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        """response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password),
                        )
                        if response.raise_for_status() is None:
                            updatedJSON = json.loads(response.text)
                            updatedJSON["data"]["relationships"].pop("client")
                            with open(f"./SPASE_JSONs/{path}.json", "w") as f:
                                json.dump(updatedJSON, f, indent=3)"""
                elif confirmation.lower() == 'no' or confirmation.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
        # ask user what to do with draft JSONs already populated w DOIs
        if draftDOIs:
            print(f"DOI already minted for these unpublished records: {draftDOIs}")
            incorrectInput = True
            publish = False
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(draftDOIs)} records? (cannot be undone). ")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) w DOI(s)")
                    publish = True
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                    # update JSON draft on DataCite (do not add "event:publish" to payload)
                    print("Updating metadata JSON(s)")
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                for key, val in draftDOIs.items():
                    protocol, domain, path = key.partition("hpde.io/")
                    with open(f"./SPASE_JSONs/{path}.json", 'r') as desiredFile:
                        data = desiredFile.read()
                    data = json.loads(data)
                    #if publish:
                        #data["attributes"]["event"] = "publish"
                    response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password)
                        )
                    if response.raise_for_status() is None:
                        updatedJSON = json.loads(response.text)
                        updatedJSON["data"]["relationships"].pop("client")
                        with open(f"./SPASE_JSONs/{path}.json", "w") as f:
                            json.dump(updatedJSON, f, indent=3)
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record
        # ask user what to do with JSONs with newly minted DOIs
        if newDOIs:
            print(f"DOIs were just minted for these unpublished records: {newDOIs}")
            incorrectInput = True
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(newDOIs)} records? (cannot be undone).")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) w DOI(s)")
                    for key, val in newDOIs.items():
                        protocol, domain, path = key.partition("hpde.io/")
                        with open(f"./SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        #data["attributes"]["event"] = "publish"
                        """response = requests.put(
                                'https://api.datacite.org/dois/{val}',
                                headers=headers,
                                json=data,
                                auth=(user, password)
                            )
                        if response.raise_for_status() is None:
                            updatedJSON = json.loads(response.text)
                            updatedJSON["data"]["relationships"].pop("client")
                            with open(f"./SPASE_JSONs/{path}.json", "w") as f:
                                json.dump(updatedJSON, f, indent=3)"""
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record

# if have path of XML file(s) in local machine
# test directories
#folder = "C:/Users/zboquet/NASA/DisplayData"
#folder = "C:/Users/zboquet/NASA/NumericalData"
folder = "C:/Users/zboquet/NASA/NumericalData/MMS/4/HotPlasmaCompositionAnalyzer/Burst/Level2/Ion"
main(folder, False)
# if just have ResourceID's
updateList = ["NASA/DisplayData/ParkerSolarProbe/WISPR/PNG/PT30M"]
#, "NASA/DisplayData/Cluster-Salsa/WBD/DS/PT30S"]
# bad ex (No creators) "NASA/DisplayData/Coriolis/SMEI/IMAGES"]
#updateList = ["Dev/SPASE-DataCite/spase"]
#main(updateList, True)