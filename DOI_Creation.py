import requests
import json
import os
import re
from lxml import etree
import getpass
from pathlib import Path
from datetime import datetime
from DataCite_Extractions import (get_temporal, get_instrument, get_observatory,
                                    get_alternate_name, get_is_part_of,
                                    get_mentions, get_ResourceID, get_metadata_license,
                                    SPASE)

# DISCLAIMER: This script assumes you have cloned the SPASE repo in your home directory

def getPaths(entry:str, paths:list) -> list:
    """Takes the absolute path of a SPASE record directory to be walked
    to extract all SPASE records present. Returns these paths using the
    list parameter paths, which holds the absolute paths generated by
    the function.

    :param entry: A string of the absolute path of the SPASE record directory
                    to be searched/walked to find all SPASE records within.
    :param paths: A list to hold absolute paths of all SPASE records found
                    within the given directory
    :return: A list containing the absolute paths of all SPASE records found
                within the given directory.
    """
    import os
    if os.path.exists(entry):
        for root, dirs, files in os.walk(entry):
            if files:
                for file in files:
                    paths.append(root + "/" + file)
    else:
        print(entry + " does not exist")
    return paths

def format_contributor(type:str, contribInfo:dict, nameType: str = "Personal") -> dict:
    """Formats the contributor(s) or creator(s) found in the SPASE record according to
    DataCite's metadata schema.
    
    :param type: If creator, should be "Remove" since creators do not have roles.
                    Otherwise, this should be the DataCite role given to the contributor.
    :param contribInfo: The dictionary containing the extracted metadata for the creator/contributor
    :param nameType: Can be 'Personal' or 'Organizational'
    """
    if ("affiliation" in contribInfo.keys()) and ("identifier" in contribInfo.keys()):
        # if ROR was found
        if "identifier" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["identifier"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type,
                            "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                                "nameIdentifier": contribInfo["identifier"]["@id"],
                                                "nameIdentifierScheme": "ORCID"}}
    elif "affiliation" in contribInfo.keys():
        if "identifier" in contribInfo["affiliation"].keys():
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"affiliationIdentifier": contribInfo["affiliation"]["identifier"]["@id"],
                                            "affiliationIdentifierScheme": "ROR",
                                            "name": contribInfo["affiliation"]["name"],
                                            "schemeUri": "https://ror.org/"},
                            "contributorType": type}
        else:
            contributor = {"name": contribInfo["name"],
                            "nameType": nameType,
                            "affiliation": {"name": contribInfo["affiliation"]["name"]},
                            "contributorType": type}
    elif "identifier" in contribInfo.keys():
        contributor = {"name": contribInfo["name"],
                        "nameType": nameType,
                        "contributorType": type,
                        "nameIdentifiers": {"schemeUri": "https://orcid.org",
                                            "nameIdentifier": contribInfo["identifier"]["@id"],
                                            "nameIdentifierScheme": "ORCID"}}
    else:
        contributor = {"name": contribInfo["name"],
                        "nameType": nameType,
                        "contributorType": type}
    if ("givenName" in contribInfo.keys()) and ("familyName" in contribInfo.keys()):
        contributor["givenName"] = contribInfo["givenName"]
        contributor["familyName"] = contribInfo["familyName"]
    return contributor

def clean_nones(value:list | dict):
    """
    Recursively remove all None values from dictionaries and lists, and returns
    the result as a new dictionary or list.

    :param value: The dictionary or list you wish to clean empty/None values from.
    """
    if isinstance(value, list):
        return [clean_nones(x) for x in value if x is not None]
    elif isinstance(value, dict):
        return {
            key: clean_nones(val)
            for key, val in value.items()
            if val is not None
        }
    else:
        return value

def delete_draft(doi:str) -> None:
    """Deletes the DataCite metadata draft record for the given DOI.
    
    :param doi: The unique doi identifier (not including https://doi.org/)
    """
    user = input("Enter DataCite username: ")
    password = input("Enter DataCite password: ")
    url = f"https://api.datacite.org/dois/{doi}"
    response = requests.delete(url, auth=(user, password))
    if response.raise_for_status() is None:
        print(f"Successfully deleted DataCite metadata record for {doi}")
    else:
        print(response.text)

def create_payload(record:str, exists:bool) -> dict[str, dict]:
    """Takes the absolute path of a SPASE xml file and a boolean determining if the script
    needs to create a new, local JSON and returns the metadata JSON payload to be submitted 
    to DataCite.
    
    :param record: The absolute path to the SPASE xml file you wish to
                        create/update DataCite metadata for.
    :param exists: A boolean which helps script know if the SPASE record
                        already has a DOI minted.
    """

    # json format must follow this at least
    # "event": "publish" only needed if want to create DOI, omit if desiring to create a Draft record
    # must include doi prefix "10.48322" if NASA
    """{
        "data": {
            "type": "dois",
            "attributes": {
            "event": "publish",
            "prefix": "10.48322",
            "creators": [
                {
                "name": "DataCite Metadata Working Group"
                }
            ],
            "titles": [
                {
                "title": "DataCite Metadata Schema Documentation for the Publication and Citation of Research Data v4.0"
                }
            ],
            "publisher": "DataCite e.V.",
            "publicationYear": 2016,
            "types": {
                "resourceTypeGeneral": "Text"
            },
            "url": "https://example.org"
            }
        }
    }"""

    # scrape metadata for each record
    instance = SPASE(record)

    # put metadata extraction and mapping functions here
    correctCreators = []
    relatedIdentifier = {}
    temporal = {}
    cadence = {}
    format = []
    subject2 = []
    rights = []
    geoLocations = []
    fundingReference = {}
    infoURL = {}
    date = {}
    doiFound = False

    # create path to json payload
    cwd = str(Path.cwd()).replace("\\", "/")
    homeDir = str(Path.home()).replace("\\", "/")
    _, homeDir, pathToFile = record.partition(f"{homeDir}/")
    pathToFile, _, _ = pathToFile.partition(".xml")
    pathToFile, _, fileName = pathToFile.rpartition("/")

    if exists:
        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", "r") as f:
            oldData = f.read()
        oldData = json.loads(oldData)
        try:
            if 'doi' in oldData["data"]["attributes"].keys():
                doiFound = True
                doi = oldData["data"]["attributes"]["doi"]
            if 'publicationYear' in oldData["data"]["attributes"].keys():
                pubYr = oldData["data"]["attributes"]["publicationYear"]
                #assert pubYr == int((instance.get_date_published())[:4])
        # handle case when existing JSON is from before this script was made
        except KeyError:
            if 'doi' in oldData.keys():
                doiFound = True
                doi = oldData["doi"]
            if 'publicationYear' in oldData.keys():
                pubYr = oldData["publicationYear"]
                #assert pubYr == int((instance.get_date_published())[:4])
    else:
        pubYr = datetime.now().year

    # format creators according to DataCite
    creators = instance.get_creator()
    if creators:
        for each in creators:
            if (", " in each["name"] or ". " in each["name"] or
            ("givenName" in each.keys() and "familyName" in each.keys())
            or "_" in each["name"]):
                correctCreator = format_contributor("Remove", each)
            # no comma or period = not a person = organization
            else:
                correctCreator = format_contributor("Remove", each, "Organizational")
            correctCreator.pop("contributorType")
            correctCreators.append(correctCreator)
    else:
        raise ValueError("No creators were found. A DOI cannot be made.")

    # determine if NumericalData or DisplayData for resourceType property
    ResourceID = get_ResourceID(instance.metadata, instance.namespaces)
    if "NumericalData" in ResourceID:
        resourceType = "NumericalData"
    else:
        resourceType = "DisplayData"

    # required fields
    payload = {
        "data": {
            "type": "dois",
            "attributes": {
            "prefix": "10.48322",
            "creators": correctCreators,
            "titles": [
                {
                "lang": "en",
                "title": instance.get_name()
                }
            ],
            "publisher": instance.get_publisher(),
            "publicationYear": pubYr,
            "types": {
                "resourceType": resourceType,
                "resourceTypeGeneral": "Dataset"
            },
            "url": instance.get_id()
            }
        }
    }

    #optional fields
    description = {"lang": "en",
                    "description": instance.get_description(),
                    "descriptionType": "Abstract"}
    if get_alternate_name(instance.metadata) is not None:
        alternativeTitle = get_alternate_name(instance.metadata)
        payload["data"]["attributes"]["titles"].append({"lang": "en",
                                            "title": alternativeTitle,
                                            "titleType": "AlternativeTitle"})
    if instance.get_same_as() is not None:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)},
                                {"alternateIdentifierType": "Prior SPASE ResourceID",
                                "alternateIdentifier": instance.get_same_as()}]
    else:
        alternateIdentifier = [{"alternateIdentifierType": "SPASE ResourceID",
                                "alternateIdentifier": get_ResourceID(instance.metadata, instance.namespaces)}]
    if instance.get_license() is not None:
        rightsDict = instance.get_license()
        for each in rightsDict:
            rights.append({"rights": each["name"],
                        "rightsUri": each["rightsURI"],
                        "schemeUri": each["schemeURI"],
                        "rightsIdentifier": each["rightsIdentifier"],
                        "rightsIdentifierScheme": each["rightsIdentifierScheme"],
                        "lang": "en"})
    if not rights:
        rights = None

    subject = instance.get_keywords()
    subjects = []
    for key, val in subject.items():
        if key == "keywords":
            for each in val:
                subjects.append({"subject": each})
        # measurementTypes require extra info
        else:
            for each in val:
                # separate words with spaces for subject field
                res = re.split(r"(?=[A-Z])", each)
                pretty_name = " ".join(filter(None, res))
                # create subject entry
                subjects.append({"subject": pretty_name,
                                    "subjectScheme": "SPASE MeasurementType",
                                    "schemeUri": "https://spase-group.org/data/model/spase-latest/spase-latest_xsd.htm#MeasurementType",
                                    "classificationCode": each})

    dates = []
    if instance.get_temporal_coverage() is not None:
        date["coverage"] = instance.get_temporal_coverage()
        temporal = {"date": date["coverage"],
                    "dateType": "Coverage"}
        dates.append(temporal)
    """if get_temporal(instance.metadata, instance.namespaces) is not None:
        date["other"] = get_temporal(instance.metadata, instance.namespaces)[1]
        cadence = {"date": date["other"],
                "dateType": "Other",
                "dateInformation": "Cadence of successive measurements"}
        dates.append(cadence)"""
    #date["updated"] = instance.get_date_modified()
    #dateModified = {"date": date["updated"],
    #                "dateType": "Updated"}
    #dates.append(dateModified)

    if instance.get_spatial_coverage() is not None:
        observedRegions = instance.get_spatial_coverage()
        # add to geoLocation and subject
        for each in observedRegions:
            geoLocations.append({"geoLocationPlace": each["name"]})
            subjects.append({"subject": each["name"],
                                    "subjectScheme": "SPASE ObservedRegion",
                                    "schemeUri": each["keywords"]["inDefinedTermSet"]["@id"],
                                    "classificationCode": each["keywords"]["termCode"]})


    for each in instance.get_potential_action():
        if each["target"]["contentType"] not in format:
            format.append(each["target"]["contentType"])
    language = "en"

    # if DOI, put this in relatedIdentifierType, else put URL
    # @type stored in resourceTypeGeneral
    relatedIdentifiers = []
    if instance.get_citation() is not None:
        for each in instance.get_citation():
            #print(each["url"])
            try:
                res = requests.get(each["url"], timeout=5)
                if res.raise_for_status() is None:
                    infoURL = {"relationType": "References",
                                "relatedIdentifier": each["url"],
                                "relatedIdentifierType": "URL",
                                "resourceTypeGeneral": "Other/website"}
                    relatedIdentifiers.append(infoURL)
            except requests.exceptions.HTTPError as errh:
                # possibly a restricted link (e.g. 401 Unauthorized)
                print(f"HTTP Error: {errh} for {each['url']}")
            except requests.exceptions.ConnectionError as errc:
                # network problem such as DNS failure, refused connection, etc
                print(f"Error Connecting: {errc} for {each['url']}")
            except requests.exceptions.Timeout as errt:
                # retry?
                print(f"Timeout Error: {errt} for {each['url']}")
            except requests.exceptions.RequestException as err:
                # something else happened
                print(f"Something went wrong for {each['url']}")
    if instance.get_is_based_on() is not None:
        relatedIdentifier["isDerivedFrom"] = instance.get_is_based_on()
        for each in relatedIdentifier["isDerivedFrom"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsDerivedFrom",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if instance.get_was_revision_of() is not None:
        relatedIdentifier["isVersionOf"] = instance.get_was_revision_of()
        for each in relatedIdentifier["isVersionOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsNewVersionOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_is_part_of(instance.metadata) is not None:
        relatedIdentifier["isPartOf"] = get_is_part_of(instance.metadata)
        for each in relatedIdentifier["isPartOf"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "IsPartOf",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_mentions(instance.metadata) is not None:
        relatedIdentifier["References"] = get_mentions(instance.metadata)
        for each in relatedIdentifier["References"]:
            if 'doi' in each["@id"]:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI",
                                "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "DOI"})
            else:
                if "@type" in each.keys():
                    relatedIdentifiers.append({"relationType": "References",
                                    "relatedIdentifier": each["@id"],
                                    "relatedIdentifierType": "URL",
                                    "resourceTypeGeneral": each["@type"]})
                else:
                    relatedIdentifiers.append({"relationType": "References",
                                "relatedIdentifier": each["@id"],
                                "relatedIdentifierType": "URL"})
    if get_instrument(instance.metadata, record) is not None:
        instruments = get_instrument(instance.metadata, record)
        for each in instruments:
            try:
                res = requests.get(each["@id"], timeout=5)
                if res.raise_for_status() is None:
                    ins = {"relationType": "IsCollectedBy",
                            "resourceTypeGeneral": "Instrument",
                            "relatedIdentifier": each["@id"],
                            "relatedIdentifierType": "URL"}
                    relatedIdentifiers.append(ins)
            except requests.exceptions.ConnectionError as err:
                print(err)

    if instance.get_id() is not None:
        metadataSchemeURI = "https://spase-group.org/data/model/spase-latest/spase-latest_xsd.htm"
        metadata = {"relationType": "HasMetadata",
                "relatedMetadataScheme": "SPASE",
                "relatedIdentifier": instance.get_id(),
                "relatedIdentifierType": "URL",
                "schemeURI": metadataSchemeURI,
                "schemeType": "xsd"}
        relatedIdentifiers.append(metadata)
    """if get_observatory(instance.metadata, record) is not None:
        observatories = get_observatory(instance.metadata, record)
        for each in observatories:
            try:
                res = requests.get(each["@id"], timeout=5)
                if res.raise_for_status() is None:
                    obs = {"relationType": "IsCollectedBy",
                            "resourceTypeGeneral": "Project",
                            "relatedIdentifier": each["@id"],
                            "relatedIdentifierType": "URL"}
                    relatedIdentifiers.append(obs)
            except requests.exceptions.ConnectionError as err:
                print(err)"""
    if not relatedIdentifiers:
        relatedIdentifiers = None

    # add funder as a fundingReference
    if instance.get_funding() is not None:
        fundingReference = []
        for each in instance.get_funding():
            # most basic entry into fundingReference
            entry = {"funderName": each["funder"]["name"],
                        "awardTitle": each["name"]}
            if "@id" in each["funder"].keys():
                entry["funderIdentifier"] = each["funder"]["@id"]
                entry["funderIdentifierType"] = "ROR"
            if "identifier" in each.keys():
                entry["awardNumber"] = each["identifier"]
            fundingReference.append(entry)

    # 8 (all in Numerical) 'contributor' roles in Contacts
    # ex: NASA/NumericalData/MMS/1/FIELDS/FGM/Burst/Level2/PT0.0078125S
    contribs = []
    contributors = instance.get_contributor()
    for each in contributors:
        # format contrib according to DataCite
        if (", " in each["contributor"]["name"] or ". " in each["contributor"]["name"] or
            ("givenName" in each["contributor"].keys() and "familyName" in each["contributor"].keys())
            or "_" in each["contributor"]["name"]):
            contributor = format_contributor(each["termCode"], each["contributor"])
        else:
            contributor = format_contributor(each["termCode"], each["contributor"], "Organizational")
        contribs.append(contributor)

    # add nonempty optional fields to json
    optionals = {"descriptions": [description],
                    "alternateIdentifiers": alternateIdentifier,
                    "rightsList": rights,
                    "relatedIdentifiers": relatedIdentifiers,
                    "subjects": subjects,
                    "dates": dates,
                    "geoLocations": geoLocations,
                    "fundingReferences": fundingReference,
                    "contributors": contribs,
                    "formats": format,
                    "language": language}
    optionals = clean_nones(optionals)

    if doiFound:
        optionals["doi"] = doi

    payload["data"]["attributes"].update(optionals)
    # json payload file to be sent to DataCite to create entry
    try:
        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", "w") as f:
            json.dump(payload, f, indent=3, sort_keys=True)
    except FileNotFoundError as err:
        pass

    return payload

def main(folders:str|list, IDsProvided:bool) -> None:
    """Takes a path to a directory containing SPASE records or the
    ResourceID(s) to the specific xml file(s) you wish to create/update
    DataCite DOI metadata records for, along with a boolean helping the
    script determine if the value provided was a folder or file(s). With
    this, the script iterates through the SPASE record(s), extracts desired
    metadata, and formats this metadata into DataCite metadata records (as JSON
    files). The script gives the user the option whether or not to publish the
    updated/new DataCite metadata record to their account. Regardless of this
    decision, these DataCite metadata records are saved in a local directory
    for user validation.
    
    :param folders: A string value of either a path to the SPASE directory or
                        the ResourceID(s) to the file(s) you wish to update/create
                        DataCite metadata records for.
    :param IDsProvided: A boolean which helps the script recognize the value provided
                            for folders as a directory or file(s).
    """
    # list that holds SPASE records already checked
    searched = []
    SPASE_paths = []
    newDOIs = {}
    publishedDOIs = {}
    draftDOIs = {}
    homeDir = str(Path.home()).replace("\\", "/")
    cwd = str(Path.cwd()).replace("\\", "/")

    # request user for DataCite login info
    user = getpass.getpass("Enter DataCite username: ")
    password = getpass.getpass("Enter DataCite password: ")

    if IDsProvided:
        # obtains all filepaths to all SPASE records found in the given list of ResourceIDs
        for folder in folders:
            #print(folder)
            # if the test record was passed
            if cwd in folder:
                formattedRecord = folder.replace("spase://", "")
            # if a SPASE record outside of this package is passed
            else:
                formattedRecord = homeDir + "/" + folder.replace("spase://", "")
            if not formattedRecord.endswith('.xml'):
                formattedRecord += ".xml"
            SPASE_paths.append(formattedRecord)
    else:
        SPASE_paths = getPaths(folders, SPASE_paths)
    #print("You entered " + folder)
    if len(SPASE_paths) == 0:
        print("No records found. Returning.")
    else:
        #print("The number of records is " + str(len(SPASE_paths)))
        # iterate through all SPASE records
        for r, record in enumerate(SPASE_paths):
            if record not in searched:
                exists = False
                #print(record)
                # make file reflect the change made in this script
                *_, pathToFile = record.partition(f"{homeDir}/")
                pathToFile, _, _ = pathToFile.partition(".xml")
                pathToFile, _, fileName = pathToFile.rpartition("/")
                try:
                    os.makedirs(f"{cwd}/SPASE_JSONs/{pathToFile}")
                except FileExistsError:
                    # check if there has already been a payload created for this record
                    testList = []
                    testList = getPaths(f"{cwd}/SPASE_JSONs/{pathToFile}", testList)
                    #print(testList)
                    if f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json" in testList:
                        exists = True

                # print message for user
                statusMessage = f"Creating DOI for record {r+1}"
                statusMessage += f" of {len(SPASE_paths)}"
                print(statusMessage)
                print(record)
                print()
                instance = SPASE(record)

                # add record to searched
                searched.append(record)

                # mint DOI
                headers = {
                    'Content-Type': 'application/vnd.api+json'
                }
                #print(exists)
                data = create_payload(record, exists)
                link = data["data"]["attributes"]["url"]
                #print(link)

                # check to see if simply updating metadata or creating a new DOI
                # if DOI in SPASE = JSON already present on DataCite = update
                if 'doi' in instance.get_url():
                    protocol, domain, doi = instance.get_url().partition("doi.org/")
                    publishedDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # DOI in local JSON means it is a draft = publish or update metadata
                elif 'doi' in data["data"]["attributes"].keys():
                    doi = data["data"]["attributes"]["doi"]
                    draftDOIs[link] = doi
                    # remove DataCite login info from JSON
                    if 'relationships' in data["data"].keys():
                        data["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", 'w') as desiredFile:
                            json.dump(data, desiredFile, indent=3, sort_keys=True)
                # means no DOI in SPASE record or draft = make one
                else:
                    # request to create DOI
                    print("No DOI exists for this record yet. Minting now.")
                    print()
                    response = requests.post(
                        'https://api.datacite.org/dois',
                        headers=headers,
                        json=data,
                        auth=(user, password)
                    )
                    if response.raise_for_status() is None:
                        # add doi to local JSON
                        newJSON = json.loads(response.text)
                        newJSON["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{pathToFile}/{fileName}.json", "w") as f:
                            json.dump(newJSON, f, indent=3)
                        doi = json.loads(response.text)["data"]["attributes"]["doi"]
                        #print(doi)
                        newDOIs[link] = doi
                    #print(json.loads(response.text))
        # ask user what to do with records already minted
        if publishedDOIs:
            incorrectInput = True
            while incorrectInput:
                confirmation = input("There is already a published JSON for the following "
                                        f"{len(publishedDOIs)} records: {publishedDOIs} "
                                        "Are you sure you wish to overwrite/update this metadata? ")
                if (confirmation.lower() == 'yes') or (confirmation.lower() == 'y'):
                    incorrectInput = False
                    # request to update DOI
                    for key, val in publishedDOIs.items():
                        protocol, domain, path = key.partition("hpde.io/")
                        with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        """response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password),
                        )
                        if response.raise_for_status() is None:
                            updatedJSON = json.loads(response.text)
                            updatedJSON["data"]["relationships"].pop("client")
                            with open(f"{cwd}/SPASE_JSONs/{path}.json", "w") as f:
                                json.dump(updatedJSON, f, indent=3)"""
                elif confirmation.lower() == 'no' or confirmation.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
        # ask user what to do with draft JSONs already populated w DOIs
        if draftDOIs:
            print(f"DOI already minted for these unpublished records: {draftDOIs}")
            incorrectInput = True
            publish = False
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(draftDOIs)} records? (cannot be undone). ")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) w DOI(s)")
                    publish = True
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                    # update JSON draft on DataCite (do not add "event:publish" to payload)
                    print("Updating metadata JSON(s)")
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                for key, val in draftDOIs.items():
                    protocol, domain, path = key.partition("hpde.io/")
                    with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                        data = desiredFile.read()
                    data = json.loads(data)
                    #if publish:
                        #data["attributes"]["event"] = "publish"
                    response = requests.put(
                            f'https://api.datacite.org/dois/{val}',
                            headers=headers,
                            json=data,
                            auth=(user, password)
                        )
                    if response.raise_for_status() is None:
                        updatedJSON = json.loads(response.text)
                        updatedJSON["data"]["relationships"].pop("client")
                        with open(f"{cwd}/SPASE_JSONs/{path}.json", "w") as f:
                            json.dump(updatedJSON, f, indent=3)
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record
        # ask user what to do with JSONs with newly minted DOIs
        if newDOIs:
            print(f"DOIs were just minted for these unpublished records: {newDOIs}")
            incorrectInput = True
            while incorrectInput:
                answer = input(f"Would you like to publish the JSONs for these {len(newDOIs)} records? (cannot be undone).")
                if (answer.lower() == 'yes') or (answer.lower() == 'y'):
                    incorrectInput = False
                    # add 'event: publish' to payload to update state from draft to findable
                    print("Publishing new JSON(s) w DOI(s)")
                    for key, val in newDOIs.items():
                        protocol, domain, path = key.partition("hpde.io/")
                        with open(f"{cwd}/SPASE_JSONs/{path}.json", 'r') as desiredFile:
                            data = desiredFile.read()
                        data = json.loads(data)
                        #data["attributes"]["event"] = "publish"
                        """response = requests.put(
                                'https://api.datacite.org/dois/{val}',
                                headers=headers,
                                json=data,
                                auth=(user, password)
                            )
                        if response.raise_for_status() is None:
                            updatedJSON = json.loads(response.text)
                            updatedJSON["data"]["relationships"].pop("client")
                            with open(f"{cwd}/SPASE_JSONs/{path}.json", "w") as f:
                                json.dump(updatedJSON, f, indent=3)"""
                elif answer.lower() == 'no' or answer.lower() == 'n':
                    incorrectInput = False
                else:
                    print("Please enter 'yes'/'no' or 'y/n'.")
                # TODO: if publishing: add call to SPASE corrections script to add PubInfo and DOI to SPASE record

# allow calls from the command line
if __name__ == "__main__":
    from sys import argv

    if len(argv) == 1:
        #HOME_DIR = str(Path.home()).replace("\\", "/")
        cwd = str(Path.cwd()).replace("\\", "/")
        print(help(main))
        print()
        print(
            "Rerun the script again, passing the SPASE repo (or a specific folder within it)"
            " that you want to create/update DataCite JSONs for as an argument followed by 'False'. " \
            "You can instead provide a string of comma-separated SPASE ID(s) if you have specific " \
            "records in mind, followed by 'True'."
        )
        print()
        print(
            "An example NASA SPASE record to see how this script works is found in this folder: "
            f"{cwd}/ExternalSPASE_XMLs/spase. To create/update the DataCite record for this example SPASE record,"
            f" run this command: `python {cwd}/DOI_Creation.py '{cwd}/ExternalSPASE_XMLs/spase' True`."
        )
    else:
        if "\\" in str(argv[1]):
            argv[1] = argv[1].replace("\\", "/")
        # if a SPASE folder/directory is given
        if not argv[2] == "True":
            main(argv[1], argv[2] == "True")
        # if a list of SPASE records are given
        else:
            # if multiple files are given
            if ", " in str(argv[1]):
                record = str(argv[1]).split(', ')
            # only one particular record was given
            else:
                record = [argv[1]]
            #print(f"arg1 is {record} and arg2 is {argv[2] == 'True'}")
            main(record, (argv[2] == "True"))

# if have path of XML file(s) in local machine
# test directories
#folder = "C:/Users/zboquet/NASA/DisplayData"
#folder = "C:/Users/zboquet/NASA/NumericalData"
"""folder = "C:/Users/zboquet/NASA/NumericalData/MMS/4/HotPlasmaCompositionAnalyzer/Burst/Level2/Ion"
#main(folder, False)

# if just have ResourceID's
#updateList = ["NASA/DisplayData/ParkerSolarProbe/WISPR/PNG/PT30M", "NASA/DisplayData/Cluster-Salsa/WBD/DS/PT30S"]
updateList = ["NASA/NumericalData/PUNCH/NFI/Level0/CR4/PT8M",
"NASA/NumericalData/PUNCH/NFI/Level0/PM4/PT4M",
"NASA/NumericalData/PUNCH/NFI/Level0/PP4/PT4M",
"NASA/NumericalData/PUNCH/NFI/Level0/PZ4/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/CR1/PT8M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PM1/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PP1/PT4M",
"NASA/NumericalData/PUNCH/WFI/1/Level0/PZ1/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/CR2/PT8M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PM2/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PP2/PT4M",
"NASA/NumericalData/PUNCH/WFI/2/Level0/PZ2/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/CR3/PT8M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PM3/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PP3/PT4M",
"NASA/NumericalData/PUNCH/WFI/3/Level0/PZ3/PT4M"
]
# bad ex (No creators) "NASA/DisplayData/Coriolis/SMEI/IMAGES"]

updateList = ["Dev/SPASE-DataCite/ExternalSPASE_XMLs/spase"]
updateList = ["NASA/NumericalData/SDO/AIA/PT12S"]
main(updateList, True)"""
